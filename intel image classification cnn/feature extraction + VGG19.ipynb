{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello!\n",
    "\n",
    "In this kernel, i will show you my solution using tensorflow. The task is to classify up to 6 class labels. First of all i get low level feature extracter layers from VGG19 pretrained model, then using fine tuning method add several own fully connected layers with prediction layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from numpy import save\n",
    "from numpy import load\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Input, Model\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data loading and data augmentation using Image data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use for augmentation : rotation, shifting to height and width, and flipping images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = \"G:/Desktop/intel image classification/seg_train/seg_train/\"\n",
    "testing_path = \"G:/Desktop/intel image classification/seg_test/seg_test/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have 14034 images for training, 3000 images for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dindar\\anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:336: UserWarning: This ImageDataGenerator specifies `zca_whitening`, which overrides setting of `featurewise_center`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14034 images belonging to 6 classes.\n",
      "Found 3000 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255,\n",
    "                                                           zca_whitening=True,\n",
    "                                                           rotation_range=40,\n",
    "                                                           width_shift_range=0.15,\n",
    "                                                           height_shift_range=0.15,\n",
    "                                                           horizontal_flip=True\n",
    "                                                           )\n",
    "training_instances = generator.flow_from_directory(training_path, \n",
    "                                                   target_size=(150, 150),\n",
    "                                                   batch_size=64)\n",
    "\n",
    "generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\n",
    "test_instances = generator.flow_from_directory(testing_path,\n",
    "                                               target_size=(150, 150),\n",
    "                                               batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading pretrained model from internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With include_top=False parameter i get model without fully connected layers and prediction layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (150, 150, 3)\n",
    "\n",
    "base_model = tf.keras.applications.VGG19(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# in the beginning i will fit only top layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 0\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added fully connected layers with prediction softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.get_layer('block3_pool').output\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "prediction = Dense(6, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=base_model.input, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For learning rate set 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 82944)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               42467840  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 44,926,278\n",
      "Trainable params: 42,600,710\n",
      "Non-trainable params: 2,325,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dindar\\anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "C:\\Users\\Dindar\\anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:735: UserWarning: This ImageDataGenerator specifies `zca_whitening`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 219 steps, validate for 94 steps\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dindar\\anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "C:\\Users\\Dindar\\anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:735: UserWarning: This ImageDataGenerator specifies `zca_whitening`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 88s 403ms/step - loss: 111.8320 - accuracy: 0.5895 - val_loss: 14.9559 - val_accuracy: 0.7670\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 87s 399ms/step - loss: 13.9940 - accuracy: 0.7041 - val_loss: 6.6847 - val_accuracy: 0.7713\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 87s 397ms/step - loss: 8.0017 - accuracy: 0.7061 - val_loss: 4.2202 - val_accuracy: 0.7670\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 89s 405ms/step - loss: 3.4411 - accuracy: 0.7175 - val_loss: 1.7073 - val_accuracy: 0.8010\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 86s 395ms/step - loss: 1.9006 - accuracy: 0.7361 - val_loss: 0.9830 - val_accuracy: 0.8170\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 87s 398ms/step - loss: 1.3213 - accuracy: 0.7419 - val_loss: 0.9808 - val_accuracy: 0.7857\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 88s 400ms/step - loss: 1.1389 - accuracy: 0.7452 - val_loss: 0.9827 - val_accuracy: 0.7757\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 87s 396ms/step - loss: 1.0401 - accuracy: 0.7468 - val_loss: 0.7391 - val_accuracy: 0.8123\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 91s 414ms/step - loss: 0.8891 - accuracy: 0.7536 - val_loss: 0.7254 - val_accuracy: 0.8060\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 95s 436ms/step - loss: 0.9239 - accuracy: 0.7470 - val_loss: 0.7326 - val_accuracy: 0.8147\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 94s 429ms/step - loss: 0.8876 - accuracy: 0.7503 - val_loss: 0.7177 - val_accuracy: 0.8077\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 94s 428ms/step - loss: 0.7881 - accuracy: 0.7687 - val_loss: 0.7438 - val_accuracy: 0.7640\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 94s 428ms/step - loss: 0.8581 - accuracy: 0.7576 - val_loss: 0.5832 - val_accuracy: 0.8133\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 93s 425ms/step - loss: 0.8893 - accuracy: 0.7480 - val_loss: 1.1034 - val_accuracy: 0.7433\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 89s 406ms/step - loss: 1.0144 - accuracy: 0.7341 - val_loss: 0.6819 - val_accuracy: 0.8167\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 86s 391ms/step - loss: 0.8644 - accuracy: 0.7503 - val_loss: 0.9667 - val_accuracy: 0.7770\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 93s 423ms/step - loss: 1.5467 - accuracy: 0.6972 - val_loss: 1.3832 - val_accuracy: 0.7247\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 96s 439ms/step - loss: 0.9364 - accuracy: 0.7466 - val_loss: 0.7761 - val_accuracy: 0.7943\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 94s 428ms/step - loss: 1.2591 - accuracy: 0.6932 - val_loss: 0.8592 - val_accuracy: 0.7707\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 93s 424ms/step - loss: 0.9489 - accuracy: 0.7309 - val_loss: 0.7826 - val_accuracy: 0.7723\n",
      "Wall time: 30min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Batch_size=64\n",
    "initial_epochs = 20\n",
    "model_fit=model.fit(training_instances, \n",
    "                    steps_per_epoch=14034 // Batch_size,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=test_instances\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "94/94 [==============================] - 12s 127ms/step - loss: 0.7826 - accuracy: 0.7723\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.782577098525585, 0.7723333]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 20 epoch accuracy equal to 77%.\n",
    "\n",
    "Then apply fine tuning for last 9 layer.\n",
    "\n",
    "But learning rate will 10 time less than previous value. Because, pretrained model weights already will be closer to desired weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers in the base model:  22\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 7\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable =  False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 82944)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               42467840  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 44,926,278\n",
      "Trainable params: 44,666,118\n",
      "Non-trainable params: 260,160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 220 steps, validate for 94 steps\n",
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dindar\\anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "C:\\Users\\Dindar\\anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:735: UserWarning: This ImageDataGenerator specifies `zca_whitening`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 106s 482ms/step - loss: 0.7124 - accuracy: 0.7785 - val_loss: 0.6183 - val_accuracy: 0.7903\n",
      "Epoch 21/30\n",
      "220/220 [==============================] - 106s 481ms/step - loss: 0.5025 - accuracy: 0.8185 - val_loss: 0.4129 - val_accuracy: 0.8577\n",
      "Epoch 22/30\n",
      "220/220 [==============================] - 105s 477ms/step - loss: 0.4606 - accuracy: 0.8339 - val_loss: 0.4071 - val_accuracy: 0.8613\n",
      "Epoch 23/30\n",
      "220/220 [==============================] - 105s 478ms/step - loss: 0.4177 - accuracy: 0.8544 - val_loss: 0.3927 - val_accuracy: 0.8607\n",
      "Epoch 24/30\n",
      "220/220 [==============================] - 159s 720ms/step - loss: 0.4063 - accuracy: 0.8535 - val_loss: 0.3507 - val_accuracy: 0.8817\n",
      "Epoch 25/30\n",
      "220/220 [==============================] - 248s 1s/step - loss: 0.3832 - accuracy: 0.8626 - val_loss: 0.3284 - val_accuracy: 0.8873\n",
      "Epoch 26/30\n",
      "220/220 [==============================] - 366s 2s/step - loss: 0.3565 - accuracy: 0.8754 - val_loss: 0.3165 - val_accuracy: 0.8907\n",
      "Epoch 27/30\n",
      "220/220 [==============================] - 361s 2s/step - loss: 0.3441 - accuracy: 0.8781 - val_loss: 0.3802 - val_accuracy: 0.8653\n",
      "Epoch 28/30\n",
      "220/220 [==============================] - 276s 1s/step - loss: 0.3448 - accuracy: 0.8781 - val_loss: 0.3048 - val_accuracy: 0.8957\n",
      "Epoch 29/30\n",
      "220/220 [==============================] - 105s 476ms/step - loss: 0.3371 - accuracy: 0.8792 - val_loss: 0.3218 - val_accuracy: 0.8917\n",
      "Epoch 30/30\n",
      "220/220 [==============================] - 105s 476ms/step - loss: 0.3147 - accuracy: 0.8873 - val_loss: 0.3484 - val_accuracy: 0.8817\n",
      "Wall time: 34min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fine_tune_epochs = 10\n",
    "total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "history_fine = model.fit(training_instances,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch =  model_fit.epoch[-1],\n",
    "                         validation_data=test_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 0.3484 - accuracy: 0.8817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.34836831268794993, 0.88166666]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('G:/Desktop/intel image classification/88%.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 220 steps, validate for 94 steps\n",
      "Epoch 30/40\n",
      "220/220 [==============================] - 104s 474ms/step - loss: 0.3101 - accuracy: 0.8879 - val_loss: 0.3269 - val_accuracy: 0.8890\n",
      "Epoch 31/40\n",
      "220/220 [==============================] - 105s 475ms/step - loss: 0.2896 - accuracy: 0.8972 - val_loss: 0.2925 - val_accuracy: 0.8990\n",
      "Epoch 32/40\n",
      "220/220 [==============================] - 105s 477ms/step - loss: 0.2924 - accuracy: 0.8982 - val_loss: 0.2834 - val_accuracy: 0.8990\n",
      "Epoch 33/40\n",
      "220/220 [==============================] - 105s 476ms/step - loss: 0.2767 - accuracy: 0.9010 - val_loss: 0.3354 - val_accuracy: 0.8873\n",
      "Epoch 34/40\n",
      "220/220 [==============================] - 105s 476ms/step - loss: 0.2820 - accuracy: 0.8999 - val_loss: 0.3528 - val_accuracy: 0.8837\n",
      "Epoch 35/40\n",
      "220/220 [==============================] - 105s 476ms/step - loss: 0.2726 - accuracy: 0.9003 - val_loss: 0.2827 - val_accuracy: 0.9037\n",
      "Epoch 36/40\n",
      "220/220 [==============================] - 105s 476ms/step - loss: 0.2607 - accuracy: 0.9072 - val_loss: 0.3076 - val_accuracy: 0.8927\n",
      "Epoch 37/40\n",
      "220/220 [==============================] - 105s 475ms/step - loss: 0.2511 - accuracy: 0.9088 - val_loss: 0.3183 - val_accuracy: 0.9040\n",
      "Epoch 38/40\n",
      "220/220 [==============================] - 174s 793ms/step - loss: 0.2516 - accuracy: 0.9099 - val_loss: 0.3073 - val_accuracy: 0.8987\n",
      "Epoch 39/40\n",
      "220/220 [==============================] - 195s 886ms/step - loss: 0.2492 - accuracy: 0.9089 - val_loss: 0.3221 - val_accuracy: 0.8960\n",
      "Epoch 40/40\n",
      "220/220 [==============================] - 192s 873ms/step - loss: 0.2513 - accuracy: 0.9072 - val_loss: 0.3128 - val_accuracy: 0.8903\n",
      "Wall time: 23min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fine_tune_epochs1 = 10\n",
    "total_epochs =  initial_epochs + fine_tune_epochs + fine_tune_epochs1\n",
    "\n",
    "history_fine = model.fit(training_instances,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch =  history_fine.epoch[-1],\n",
    "                         validation_data=test_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "94/94 [==============================] - 17s 181ms/step - loss: 0.3128 - accuracy: 0.8903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.31281380910188594, 0.89033335]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After first part fine tuning i will fit again with 18 last layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers in the base model:  22\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 4\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable =  False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 82944)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               42467840  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 44,926,278\n",
      "Trainable params: 44,887,558\n",
      "Non-trainable params: 38,720\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 220 steps, validate for 94 steps\n",
      "Epoch 40/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dindar\\anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "C:\\Users\\Dindar\\anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:735: UserWarning: This ImageDataGenerator specifies `zca_whitening`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 199s 906ms/step - loss: 0.2712 - accuracy: 0.9058 - val_loss: 0.3285 - val_accuracy: 0.9007\n",
      "Epoch 41/45\n",
      "220/220 [==============================] - 194s 884ms/step - loss: 0.2619 - accuracy: 0.9056 - val_loss: 0.3622 - val_accuracy: 0.8680\n",
      "Epoch 42/45\n",
      "220/220 [==============================] - 165s 752ms/step - loss: 0.2407 - accuracy: 0.9137 - val_loss: 0.2949 - val_accuracy: 0.8933\n",
      "Epoch 43/45\n",
      "220/220 [==============================] - 131s 595ms/step - loss: 0.2366 - accuracy: 0.9153 - val_loss: 0.2903 - val_accuracy: 0.9010\n",
      "Epoch 44/45\n",
      "220/220 [==============================] - 132s 598ms/step - loss: 0.2285 - accuracy: 0.9186 - val_loss: 0.2803 - val_accuracy: 0.9050\n",
      "Epoch 45/45\n",
      "220/220 [==============================] - 131s 597ms/step - loss: 0.2231 - accuracy: 0.9210 - val_loss: 0.3108 - val_accuracy: 0.9043\n",
      "Wall time: 15min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fine_tune_epochs2 = 5\n",
    "total_epochs =  initial_epochs + fine_tune_epochs + fine_tune_epochs1 + fine_tune_epochs2\n",
    "\n",
    "history_fine = model.fit(training_instances,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch =  history_fine.epoch[-1],\n",
    "                         validation_data=test_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "94/94 [==============================] - 12s 129ms/step - loss: 0.3108 - accuracy: 0.9043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3108464305467428, 0.90433335]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then i also fit model again with last 20 layers. And each step of fine tuning learning rate value less and less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers in the base model:  22\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 2\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable =  False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 82944)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               42467840  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 44,926,278\n",
      "Trainable params: 44,924,486\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 220 steps, validate for 94 steps\n",
      "Epoch 45/55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dindar\\anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "C:\\Users\\Dindar\\anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:735: UserWarning: This ImageDataGenerator specifies `zca_whitening`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 175s 796ms/step - loss: 0.1762 - accuracy: 0.9370 - val_loss: 0.2644 - val_accuracy: 0.9163\n",
      "Epoch 46/55\n",
      "220/220 [==============================] - 173s 788ms/step - loss: 0.1662 - accuracy: 0.9401 - val_loss: 0.2581 - val_accuracy: 0.9143\n",
      "Epoch 47/55\n",
      "220/220 [==============================] - 174s 789ms/step - loss: 0.1612 - accuracy: 0.9417 - val_loss: 0.2482 - val_accuracy: 0.9167\n",
      "Epoch 48/55\n",
      "220/220 [==============================] - 173s 788ms/step - loss: 0.1516 - accuracy: 0.9451 - val_loss: 0.2510 - val_accuracy: 0.9163\n",
      "Epoch 49/55\n",
      "220/220 [==============================] - 174s 792ms/step - loss: 0.1511 - accuracy: 0.9428 - val_loss: 0.2636 - val_accuracy: 0.9157\n",
      "Epoch 50/55\n",
      "220/220 [==============================] - 173s 787ms/step - loss: 0.1538 - accuracy: 0.9453 - val_loss: 0.2568 - val_accuracy: 0.9183\n",
      "Epoch 51/55\n",
      "220/220 [==============================] - 173s 786ms/step - loss: 0.1481 - accuracy: 0.9458 - val_loss: 0.2547 - val_accuracy: 0.9150\n",
      "Epoch 52/55\n",
      "220/220 [==============================] - 173s 786ms/step - loss: 0.1464 - accuracy: 0.9461 - val_loss: 0.2576 - val_accuracy: 0.9173\n",
      "Epoch 53/55\n",
      "220/220 [==============================] - 173s 786ms/step - loss: 0.1454 - accuracy: 0.9467 - val_loss: 0.2573 - val_accuracy: 0.9193\n",
      "Epoch 54/55\n",
      "220/220 [==============================] - 173s 787ms/step - loss: 0.1435 - accuracy: 0.9468 - val_loss: 0.2637 - val_accuracy: 0.9160\n",
      "Epoch 55/55\n",
      "220/220 [==============================] - 173s 785ms/step - loss: 0.1446 - accuracy: 0.9458 - val_loss: 0.2514 - val_accuracy: 0.9157\n",
      "Wall time: 31min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fine_tune_epochs3 = 10\n",
    "total_epochs =  initial_epochs + fine_tune_epochs + fine_tune_epochs1 + fine_tune_epochs2 + fine_tune_epochs3\n",
    "\n",
    "history_fine = model.fit(training_instances,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch =  history_fine.epoch[-1],\n",
    "                         validation_data=test_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 0.2514 - accuracy: 0.9157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2513879728008141, 0.91566664]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('G:/Desktop/intel image classification/91%.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of last model was 91%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
